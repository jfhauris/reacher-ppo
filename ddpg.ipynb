{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def soft_update(target, source, tau):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
    "        \n",
    "def hard_update(target, source):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(param.data)\n",
    "        \n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "env = UnityEnvironment(file_name=\"Reacher_single\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, num_inputs, action_space):\n",
    "        super(Actor, self).__init__()\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        self.mu = nn.Linear(hidden_size, action_space)\n",
    "        self.mu.weight.data.mul_(0.1)\n",
    "        self.mu.bias.data.mul_(0.1)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        x = self.ln1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.linear2(x)\n",
    "        x = self.ln2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        mu = F.tanh(self.mu(x))\n",
    "        return mu\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, num_inputs, action_space):\n",
    "        super(Critic, self).__init__()\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        self.linear2 = nn.Linear(hidden_size + action_space, hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        self.V = nn.Linear(hidden_size, 1)\n",
    "        self.V.weight.data.mul_(0.1)\n",
    "        self.V.bias.data.mul_(0.1)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = state\n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        x = self.ln1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = torch.cat((x, action), 2)\n",
    "        x = self.linear2(x)\n",
    "        x = self.ln2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        V = self.V(x)\n",
    "        return V\n",
    "    \n",
    "class DDPG(object):\n",
    "    \n",
    "    def __init__(self, gamma, tau, hidden_size, num_inputs, action_space):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        self.actor = Actor(hidden_size, num_inputs, action_space)\n",
    "        self.actor_target = Actor(hidden_size, num_inputs, action_space)\n",
    "        self.actor_perturbed = Actor(hidden_size, num_inputs, action_space)\n",
    "        self.actor_optim = Adam(self.actor.parameters(), lr=1e-3)\n",
    "        \n",
    "        self.critic = Critic(hidden_size, num_inputs, action_space)\n",
    "        self.critic_target = Critic(hidden_size, num_inputs, action_space)\n",
    "        self.critic_optim = Adam(self.critic.parameters(), lr=1e-4)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        \n",
    "        hard_update(self.actor_target, self.actor)\n",
    "        hard_update(self.critic_target, self. critic)\n",
    "        \n",
    "    def select_action(self, state, action_noise=None, param_noise=None):\n",
    "        self.actor.eval()\n",
    "        if param_noise is not None:\n",
    "            mu = self.actor_perturbed((Variable(state)))\n",
    "        else:\n",
    "            mu = self.actor((Variable(state)))\n",
    "            \n",
    "        self.actor.train()\n",
    "        mu = mu.data\n",
    "        \n",
    "        if action_noise is not None:\n",
    "            mu += torch.Tensor(action_noise.noise())\n",
    "            \n",
    "        return mu.clamp(-1, 1)\n",
    "    \n",
    "    def update_parameters(self, batch):\n",
    "        state_batch = Variable(torch.cat(batch.state))\n",
    "        action_batch = Variable(torch.cat(batch.action)).view(BATCH_SIZE, NUMBER_OF_AGENTS, ACTION_SIZE)\n",
    "\n",
    "        reward_batch = Variable(torch.cat(batch.reward))\n",
    "        mask_batch = Variable(torch.cat(batch.mask))\n",
    "        next_state_batch = Variable(torch.cat(batch.next_state))\n",
    "        \n",
    "        next_action_batch = self.actor_target(next_state_batch)\n",
    "        next_state_action_values = self.critic_target(next_state_batch, next_action_batch)\n",
    "        \n",
    "        reward_batch = reward_batch.unsqueeze(1)\n",
    "        mask_batch = mask_batch.unsqueeze(1)\n",
    "        expected_state_action_batch = reward_batch + (self.gamma * mask_batch * next_state_action_values)\n",
    "        \n",
    "        self.critic_optim.zero_grad()\n",
    "        \n",
    "        state_action_batch = self.critic((state_batch), (action_batch))\n",
    "        \n",
    "        value_loss = F.mse_loss(state_action_batch, expected_state_action_batch)\n",
    "        value_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "        \n",
    "        self.actor_optim.zero_grad()\n",
    "        \n",
    "        policy_loss = -self.critic((state_batch), self.actor((state_batch)))\n",
    "        \n",
    "        policy_loss = policy_loss.mean()\n",
    "        policy_loss.backward()\n",
    "        self.actor_optim.step()\n",
    "        \n",
    "        soft_update(self.actor_target, self.actor, self.tau)\n",
    "        soft_update(self.critic_target, self.critic, self.tau)\n",
    "        \n",
    "        return value_loss.item(), policy_loss.item()\n",
    "    \n",
    "    def perturb_actor_parameters(self, param_noise):\n",
    "        hard_update(self.actor_perturbed, self.actor)\n",
    "        params = self.actor_perturbed.state_dict()\n",
    "        for name in params:\n",
    "            if 'ln' in name:\n",
    "                pass\n",
    "            param = params[name]\n",
    "            param += torch.randn(param.shape) * param_noise.current_stddev\n",
    "            \n",
    "    def save_model(self, env_name, suffix=\"\", actor_path=None, critic_path=None):\n",
    "        if not os.path.exists('models/'):\n",
    "            os.makedirs('models/')\n",
    "            \n",
    "        if actor_path is None:\n",
    "            actor_path = f'models/ddpg_actor_{env_name}_{suffix}'\n",
    "        if critic_path is None:\n",
    "            critic_path = f'models/ddpg_critic_{env_name}_{suffix}'\n",
    "        print(f'Saving models to {actor_path} and {critic_path}')\n",
    "        torch.save(self.actor.state_dict(), actor_path)\n",
    "        torch.save(self.critic.state_dict(), critic_path)\n",
    "        \n",
    "    def load_model(self, actor_path, critic_path):\n",
    "        print(f'Loading models from {actor_path} and {critic_path}')\n",
    "        if actor_path is not None:\n",
    "            self.actor.load_state_dict(torch.load(actor_path))\n",
    "        if critic_path is not None:\n",
    "            self.criric.load_state_dict(torch.load(critic_path))\n",
    "            \n",
    "import numpy as np\n",
    "import torch\n",
    "from math import sqrt\n",
    "\n",
    "\"\"\"\n",
    "From OpenAI Baselines:\n",
    "https://github.com/openai/baselines/blob/master/baselines/ddpg/noise.py\n",
    "\"\"\"\n",
    "class AdaptiveParamNoiseSpec(object):\n",
    "    def __init__(self, initial_stddev=0.1, desired_action_stddev=0.2, adaptation_coefficient=1.01):\n",
    "        \"\"\"\n",
    "        Note that initial_stddev and current_stddev refer to std of parameter noise, \n",
    "        but desired_action_stddev refers to (as name notes) desired std in action space\n",
    "        \"\"\"\n",
    "        self.initial_stddev = initial_stddev\n",
    "        self.desired_action_stddev = desired_action_stddev\n",
    "        self.adaptation_coefficient = adaptation_coefficient\n",
    "\n",
    "        self.current_stddev = initial_stddev\n",
    "\n",
    "    def adapt(self, distance):\n",
    "        if distance > self.desired_action_stddev:\n",
    "            # Decrease stddev.\n",
    "            self.current_stddev /= self.adaptation_coefficient\n",
    "        else:\n",
    "            # Increase stddev.\n",
    "            self.current_stddev *= self.adaptation_coefficient\n",
    "\n",
    "    def get_stats(self):\n",
    "        stats = {\n",
    "            'param_noise_stddev': self.current_stddev,\n",
    "        }\n",
    "        return stats\n",
    "\n",
    "    def __repr__(self):\n",
    "        fmt = 'AdaptiveParamNoiseSpec(initial_stddev={}, desired_action_stddev={}, adaptation_coefficient={})'\n",
    "        return fmt.format(self.initial_stddev, self.desired_action_stddev, self.adaptation_coefficient)\n",
    "\n",
    "def ddpg_distance_metric(actions1, actions2):\n",
    "    \"\"\"\n",
    "    Compute \"distance\" between actions taken by two policies at the same states\n",
    "    Expects numpy arrays\n",
    "    \"\"\"\n",
    "    diff = actions1-actions2\n",
    "    mean_diff = np.mean(np.square(diff), axis=0)\n",
    "    dist = sqrt(np.mean(mean_diff))\n",
    "    return dist\n",
    "\n",
    "import random\n",
    "from collections import namedtuple\n",
    "\n",
    "# Taken from\n",
    "# https://github.com/pytorch/tutorials/blob/master/Reinforcement%20(Q-)Learning%20with%20PyTorch.ipynb\n",
    "\n",
    "Transition = namedtuple(\n",
    "    'Transition', ('state', 'action', 'mask', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "# states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "# scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "# while True:\n",
    "#     actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "#     actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "#     env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "#     next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "#     rewards = env_info.rewards                         # get reward (for each agent)\n",
    "#     dones = env_info.local_done                        # see if episode finished\n",
    "#     scores += env_info.rewards                         # update the score (for each agent)\n",
    "#     states = next_states                               # roll over states to next time step\n",
    "#     if np.any(dones):                                  # exit loop if episode finished\n",
    "#         break\n",
    "# print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUMBER_OF_AGENTS = states.shape[0]\n",
    "STATE_SIZE = state_size\n",
    "ACTION_SIZE = action_size\n",
    "\n",
    "GAMMA = 0.99\n",
    "TAU = 0.9\n",
    "HIDDEN_SIZE = STATE_SIZE * 2\n",
    "NOISE_SCALE = 0.1\n",
    "EPISODES = 500\n",
    "BATCH_SIZE = 64\n",
    "UPDATES_PER_STEP = 4\n",
    "REPLAY_SIZE = 1000000\n",
    "SEED = 33\n",
    "TMAX = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, total numsteps: 1001, reward: 0.0, average reward: 0.014999999664723873\n",
      "Episode: 10, total numsteps: 11011, reward: 0.0, average reward: 0.6849999846890569\n",
      "Episode: 20, total numsteps: 21021, reward: 1.1299999747425318, average reward: 1.257999971881509\n",
      "Episode: 30, total numsteps: 31031, reward: 0.9199999794363976, average reward: 1.924999956972897\n",
      "Episode: 40, total numsteps: 41041, reward: 1.9499999564141035, average reward: 2.1399999521672726\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "writer = SummaryWriter()\n",
    "agent = DDPG(GAMMA, TAU, HIDDEN_SIZE, STATE_SIZE, ACTION_SIZE)\n",
    "param_noise = AdaptiveParamNoiseSpec(initial_stddev=0.05, desired_action_stddev=NOISE_SCALE, adaptation_coefficient=1.05)\n",
    "memory = ReplayMemory(REPLAY_SIZE)\n",
    "\n",
    "rewards = []\n",
    "total_numsteps = 0\n",
    "updates = 0\n",
    "\n",
    "for i_episode in range(EPISODES):\n",
    "    t = 0\n",
    "    env_info = env.reset(train_mode=True)[brain_name] \n",
    "    state = torch.Tensor([env_info.vector_observations])\n",
    "    \n",
    "    agent.perturb_actor_parameters(param_noise)\n",
    "    \n",
    "    episode_reward = 0\n",
    "    \n",
    "    while True:\n",
    "        action = agent.select_action(state, None, param_noise).view(NUMBER_OF_AGENTS, ACTION_SIZE)\n",
    "        env_info = env.step(action.numpy())[brain_name]\n",
    "        next_state = env_info.vector_observations\n",
    "        reward = env_info.rewards\n",
    "        done = env_info.local_done\n",
    "        total_numsteps += 1\n",
    "        episode_reward += sum(reward) / len(reward)\n",
    "        \n",
    "        mask = torch.Tensor([not d for d in done])\n",
    "        next_state = torch.Tensor([next_state])\n",
    "        reward = torch.Tensor([reward])\n",
    "        \n",
    "        memory.push(state, action, mask, next_state, reward)\n",
    "        state = next_state\n",
    "        t += 1\n",
    "        if len(memory) > BATCH_SIZE:\n",
    "            for _ in range(UPDATES_PER_STEP):\n",
    "                transitions = memory.sample(BATCH_SIZE)\n",
    "                batch = Transition(*zip(*transitions))\n",
    "                value_loss, policy_loss = agent.update_parameters(batch)\n",
    "                writer.add_scalar('loss/value', value_loss, updates)\n",
    "                writer.add_scalar('loss/policy', policy_loss, updates)\n",
    "                \n",
    "                updates += 1\n",
    "                \n",
    "        if True in done:\n",
    "            break\n",
    "        \n",
    "    episode_transitions = memory.memory[memory.position - t:memory.position]\n",
    "    states = torch.cat([transition[0] for transition in episode_transitions], 0)\n",
    "    unperturbed_actions = agent.select_action(states, None, None)\n",
    "    perturbed_actions = torch.cat([transition[1] for transition in episode_transitions], 0)\n",
    "\n",
    "    ddpq_dist = ddpg_distance_metric(perturbed_actions.numpy(), unperturbed_actions.numpy()) * 10\n",
    "    param_noise.adapt(ddpq_dist)\n",
    "\n",
    "    rewards.append(episode_reward)\n",
    "    if i_episode % 10 == 0:\n",
    "        env_info = env.reset(train_mode=True)[brain_name] \n",
    "        state = torch.Tensor([env_info.vector_observations])\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "            action = agent.select_action(state).view(-1)\n",
    "            env_info = env.step(action.numpy())[brain_name]\n",
    "            next_state = env_info.vector_observations\n",
    "            next_state = torch.Tensor([next_state])\n",
    "            reward = env_info.rewards\n",
    "            done = env_info.local_done\n",
    "            episode_reward += sum(reward) / len(reward)\n",
    "            state = next_state\n",
    "            if True in done:\n",
    "                break\n",
    "\n",
    "        writer.add_scalar('reward/test', episode_reward, i_episode)\n",
    "        rewards.append(episode_reward)\n",
    "        print(\"Episode: {}, total numsteps: {}, reward: {}, average reward: {}\".format(i_episode, total_numsteps, rewards[-1], np.mean(rewards[-10:])))\n",
    "    if np.mean(rewards[-100:]) > 30.0:\n",
    "        agent.save_model(\"ddqn\")\n",
    "# env_info = env.reset(train_mode=True)[brain_name]\n",
    "# writt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
