{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reacher - PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from unityagents import UnityEnvironment\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from imported_utils import Batcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "DISCOUNT_RATE = 0.99\n",
    "TAU = 0.95\n",
    "GRADIENT_CLIP = 5\n",
    "ROLLOUT_LENGTH = 2048\n",
    "OPTIMIZATION_EPOCHS = 10\n",
    "PPO_CLIP = 0.02\n",
    "LOG_INTERVAL = 2048\n",
    "MAX_STEPS = 2e7\n",
    "MINI_BATCH_NUMBER = 32\n",
    "ENTROPY_COEFICENT = 0.01\n",
    "EPISODE_COUNT = 10\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = UnityEnvironment(file_name=\"Reacher\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n",
    "\n",
    "\n",
    "def layer_init(layer, w_scale=1.0):\n",
    "    nn.init.orthogonal_(layer.weight.data)\n",
    "    layer.weight.data.mul_(w_scale)\n",
    "    nn.init.constant_(layer.bias.data, 0)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class DummyBody(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(DummyBody, self).__init__()\n",
    "        self.feature_dim = state_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class ActorCriticNet(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, phi_body, actor_body, critic_body):\n",
    "        super(ActorCriticNet, self).__init__()\n",
    "        if phi_body is None: phi_body = DummyBody(state_dim)\n",
    "        if actor_body is None: actor_body = DummyBody(phi_body.feature_dim)\n",
    "        if critic_body is None: critic_body = DummyBody(phi_body.feature_dim)\n",
    "        self.phi_body = phi_body\n",
    "        self.actor_body = actor_body\n",
    "        self.critic_body = critic_body\n",
    "        self.fc_action = layer_init(nn.Linear(actor_body.feature_dim, action_dim), 1e-3)\n",
    "        self.fc_critic = layer_init(nn.Linear(critic_body.feature_dim, 1), 1e-3)\n",
    "\n",
    "        self.actor_params = list(self.actor_body.parameters()) + list(self.fc_action.parameters())\n",
    "        self.critic_params = list(self.critic_body.parameters()) + list(self.fc_critic.parameters())\n",
    "        self.phi_params = list(self.phi_body.parameters())\n",
    "\n",
    "\n",
    "class GaussianActorCriticNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 state_dim,\n",
    "                 action_dim,\n",
    "                 phi_body=None,\n",
    "                 actor_body=None,\n",
    "                 critic_body=None):\n",
    "        super(GaussianActorCriticNet, self).__init__()\n",
    "        self.network = ActorCriticNet(state_dim, action_dim, phi_body, actor_body, critic_body)\n",
    "        self.std = nn.Parameter(torch.ones(1, action_dim))\n",
    "        self.to(DEVICE)\n",
    "\n",
    "    def forward(self, obs, action=None):\n",
    "        obs = torch.Tensor(obs)\n",
    "        phi = self.network.phi_body(obs)\n",
    "        phi_a = self.network.actor_body(phi)\n",
    "        phi_v = self.network.critic_body(phi)\n",
    "        mean = F.tanh(self.network.fc_action(phi_a))\n",
    "        v = self.network.fc_critic(phi_v)\n",
    "        dist = torch.distributions.Normal(mean, self.std)\n",
    "        if action is None:\n",
    "            action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        log_prob = torch.sum(log_prob, dim=1, keepdim=True)\n",
    "        return action, log_prob, torch.Tensor(np.zeros((log_prob.size(0), 1))), v\n",
    "    \n",
    "class FCBody(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_units=(64, 64), gate=F.relu):\n",
    "        super(FCBody, self).__init__()\n",
    "        dims = (state_dim, ) + hidden_units\n",
    "        self.layers = nn.ModuleList([layer_init(nn.Linear(dim_in, dim_out)) for dim_in, dim_out in zip(dims[:-1], dims[1:])])\n",
    "        self.gate = gate\n",
    "        self.feature_dim = dims[-1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = self.gate(layer(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "policy = GaussianActorCriticNet(state_size, action_size, actor_body=FCBody(state_size), critic_body=FCBody(state_size))\n",
    "optimizier = optim.Adam(policy.parameters(), 3e-4, eps=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    \n",
    "class PPOAgent(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.network = policy\n",
    "        self.opt = optimizier\n",
    "        self.total_steps = 0\n",
    "        self.all_rewards = np.zeros(num_agents)\n",
    "        self.episode_rewards = []\n",
    "        \n",
    "        env_info = env.reset(train_mode=True)[brain_name]    \n",
    "        self.states = env_info.vector_observations              \n",
    "\n",
    "    def step(self):\n",
    "        rollout = []\n",
    "        env_info = env.reset(train_mode=True)[brain_name]    \n",
    "        self.states = env_info.vector_observations  \n",
    "        states = self.states\n",
    "        for _ in range(ROLLOUT_LENGTH):\n",
    "            actions, log_probs, _, values = self.network(states)\n",
    "            env_info = env.step(actions.cpu().detach().numpy())[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            terminals = np.array([1 if t else 0 for t in env_info.local_done])\n",
    "            self.all_rewards += rewards\n",
    "            \n",
    "            for i, terminal in enumerate(terminals):\n",
    "                if terminals[i]:\n",
    "                    self.episode_rewards.append(self.all_rewards[i])\n",
    "                    self.all_rewards[i] = 0\n",
    "                    \n",
    "            rollout.append([states, values.detach(), actions.detach(), log_probs.detach(), rewards, 1 - terminals])\n",
    "            states = next_states\n",
    "\n",
    "        self.states = states\n",
    "        pending_value = self.network(states)[-1]\n",
    "        rollout.append([states, pending_value, None, None, None, None])\n",
    "\n",
    "        processed_rollout = [None] * (len(rollout) - 1)\n",
    "        advantages = torch.Tensor(np.zeros((num_agents, 1)))\n",
    "        returns = pending_value.detach()\n",
    "        for i in reversed(range(len(rollout) - 1)):\n",
    "            states, value, actions, log_probs, rewards, terminals = rollout[i]\n",
    "            terminals = torch.Tensor(terminals).unsqueeze(1)\n",
    "            rewards = torch.Tensor(rewards).unsqueeze(1)\n",
    "            actions = torch.Tensor(actions)\n",
    "            states = torch.Tensor(states)\n",
    "            next_value = rollout[i + 1][1]\n",
    "            returns = rewards + DISCOUNT_RATE * terminals * returns\n",
    "\n",
    "            td_error = rewards + DISCOUNT_RATE * terminals * next_value.detach() - value.detach()\n",
    "            advantages = advantages * TAU * DISCOUNT_RATE * terminals + td_error\n",
    "            processed_rollout[i] = [states, actions, log_probs, returns, advantages]\n",
    "\n",
    "        states, actions, log_probs_old, returns, advantages = map(lambda x: torch.cat(x, dim=0), zip(*processed_rollout))\n",
    "        advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "\n",
    "        batcher = Batcher(states.size(0) // MINI_BATCH_NUMBER, [np.arange(states.size(0))])\n",
    "        for _ in range(OPTIMIZATION_EPOCHS):\n",
    "            batcher.shuffle()\n",
    "            while not batcher.end():\n",
    "                batch_indices = batcher.next_batch()[0]\n",
    "                batch_indices = torch.Tensor(batch_indices).long()\n",
    "                sampled_states = states[batch_indices]\n",
    "                sampled_actions = actions[batch_indices]\n",
    "                sampled_log_probs_old = log_probs_old[batch_indices]\n",
    "                sampled_returns = returns[batch_indices]\n",
    "                sampled_advantages = advantages[batch_indices]\n",
    "\n",
    "                _, log_probs, entropy_loss, values = self.network(sampled_states, sampled_actions)\n",
    "                ratio = (log_probs - sampled_log_probs_old).exp()\n",
    "                obj = ratio * sampled_advantages\n",
    "                obj_clipped = ratio.clamp(1.0 - PPO_CLIP,\n",
    "                                          1.0 + PPO_CLIP) * sampled_advantages\n",
    "                policy_loss = -torch.min(obj, obj_clipped).mean(0) - ENTROPY_COEFICENT * entropy_loss.mean()\n",
    "\n",
    "                value_loss = 0.5 * (sampled_returns - values).pow(2).mean()\n",
    "\n",
    "                self.opt.zero_grad()\n",
    "                (policy_loss + value_loss).backward()\n",
    "                nn.utils.clip_grad_norm_(self.network.parameters(), GRADIENT_CLIP)\n",
    "                self.opt.step()\n",
    "\n",
    "        steps = ROLLOUT_LENGTH * num_agents\n",
    "        self.total_steps += steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Total score this episode: 0.18999999575316906 Average 1: 0.18999999575316906\n",
      "Episode: 2 Total score this episode: 0.09399999789893627 Average 2: 0.14199999682605266\n",
      "Episode: 3 Total score this episode: 0.11999999731779099 Average 3: 0.1346666636566321\n",
      "Episode: 4 Total score this episode: 0.24599999450147153 Average 4: 0.16249999636784196\n",
      "Episode: 5 Total score this episode: 0.05649999873712659 Average 5: 0.1412999968416989\n",
      "Episode: 6 Total score this episode: 0.1169999973848462 Average 6: 0.13724999693222345\n",
      "Episode: 7 Total score this episode: 0.19499999564141035 Average 7: 0.1454999967478216\n",
      "Episode: 8 Total score this episode: 0.24299999456852675 Average 8: 0.15768749647540972\n",
      "Episode: 9 Total score this episode: 0.2699999939650297 Average 9: 0.17016666286314527\n",
      "Episode: 10 Total score this episode: 0.2319999948143959 Average 10: 0.17634999605827034\n"
     ]
    }
   ],
   "source": [
    "agent = PPOAgent()\n",
    "all_scores = []\n",
    "for i in range(EPISODES_NUMBER):\n",
    "    agent.step()\n",
    "    \n",
    "    env_info = env.reset(train_mode=True)[brain_name]    \n",
    "    states = env_info.vector_observations                 \n",
    "    scores = np.zeros(num_agents)                         \n",
    "    while True:\n",
    "        actions, log_probs, _, values = policy(states)\n",
    "        env_info = env.step(actions.cpu().detach().numpy())[brain_name]\n",
    "        next_states = env_info.vector_observations         \n",
    "        rewards = env_info.rewards                         \n",
    "        dones = env_info.local_done                     \n",
    "        scores += env_info.rewards                      \n",
    "        states = next_states                               \n",
    "        if np.any(dones):                                  \n",
    "            break\n",
    "    m = np.mean(scores)\n",
    "    all_scores.append(m)\n",
    "    \n",
    "    if len(all_scores) > 100:\n",
    "        h = np.mean(np.array(all_scores[-100:]))\n",
    "    else:\n",
    "        h = np.mean(np.array(all_scores))\n",
    "    if h > 30.0:\n",
    "        agent.save(f\"ppo-over-30-episode-{i}\")\n",
    "    print('Episode: {} Total score this episode: {} Average {}: {}'.format(i + 1, m, min(i + 1, 100), h))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
