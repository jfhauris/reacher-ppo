{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reacher - PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch import Tensor as tensor\n",
    "# import torch.multiprocessing as mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "DISCOUNT_RATE = 0.99\n",
    "TAU = 0.95\n",
    "GRADIENT_CLIP = 5\n",
    "ROLLOUT_LENGTH = 2048\n",
    "OPTIMIZATION_EPOCHS = 10\n",
    "PPO_CLIP = 0.02\n",
    "LOG_INTERVAL = 2048\n",
    "MAX_STEPS = 2e7\n",
    "MINI_BATCH_NUMBER = 32\n",
    "ENTROPY_COEFICENT = 0.01\n",
    "EPISODES_NUMBER = 10\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = UnityEnvironment(file_name=\"Reacher\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Batcher:\n",
    "    def __init__(self, batch_size, data):\n",
    "        self.batch_size = batch_size\n",
    "        self.data = data\n",
    "        self.num_entries = len(data[0])\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.batch_start = 0\n",
    "        self.batch_end = self.batch_start + self.batch_size\n",
    "\n",
    "    def end(self):\n",
    "        return self.batch_start >= self.num_entries\n",
    "\n",
    "    def next_batch(self):\n",
    "        batch = []\n",
    "        for d in self.data:\n",
    "            batch.append(d[self.batch_start: self.batch_end])\n",
    "        self.batch_start = self.batch_end\n",
    "        self.batch_end = min(self.batch_start + self.batch_size, self.num_entries)\n",
    "        return batch\n",
    "\n",
    "    def shuffle(self):\n",
    "        indices = np.arange(self.num_entries)\n",
    "        np.random.shuffle(indices)\n",
    "        self.data = [d[indices] for d in self.data]\n",
    "\n",
    "class BaseAgent:\n",
    "    def __init__(self):\n",
    "        pass \n",
    "    \n",
    "    def close(self):\n",
    "        close_obj(self.task)\n",
    "\n",
    "    def save(self, filename):\n",
    "        torch.save(self.network.state_dict(), filename)\n",
    "\n",
    "    def load(self, filename):\n",
    "        state_dict = torch.load(filename, map_location=lambda storage, loc: storage)\n",
    "        self.network.load_state_dict(state_dict)\n",
    "\n",
    "    def eval_step(self, state):\n",
    "        raise Exception('eval_step not implemented')\n",
    "\n",
    "    def eval_episode(self):\n",
    "        env = self.config.eval_env\n",
    "        state = env.reset()\n",
    "        total_rewards = 0\n",
    "        while True:\n",
    "            action = self.eval_step(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_rewards += reward\n",
    "            if done:\n",
    "                break\n",
    "        return total_rewards\n",
    "\n",
    "    def eval_episodes(self):\n",
    "        rewards = []\n",
    "        for ep in range(self.config.eval_episodes):\n",
    "            rewards.append(self.eval_episode())\n",
    "        self.config.logger.info('evaluation episode return: %f(%f)' % (\n",
    "            np.mean(rewards), np.std(rewards) / np.sqrt(len(rewards))))\n",
    "\n",
    "def layer_init(layer, w_scale=1.0):\n",
    "    nn.init.orthogonal_(layer.weight.data)\n",
    "    layer.weight.data.mul_(w_scale)\n",
    "    nn.init.constant_(layer.bias.data, 0)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class DummyBody(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(DummyBody, self).__init__()\n",
    "        self.feature_dim = state_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class ActorCriticNet(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, phi_body, actor_body, critic_body):\n",
    "        super(ActorCriticNet, self).__init__()\n",
    "        if phi_body is None: phi_body = DummyBody(state_dim)\n",
    "        if actor_body is None: actor_body = DummyBody(phi_body.feature_dim)\n",
    "        if critic_body is None: critic_body = DummyBody(phi_body.feature_dim)\n",
    "        self.phi_body = phi_body\n",
    "        self.actor_body = actor_body\n",
    "        self.critic_body = critic_body\n",
    "        self.fc_action = layer_init(nn.Linear(actor_body.feature_dim, action_dim), 1e-3)\n",
    "        self.fc_critic = layer_init(nn.Linear(critic_body.feature_dim, 1), 1e-3)\n",
    "\n",
    "        self.actor_params = list(self.actor_body.parameters()) + list(self.fc_action.parameters())\n",
    "        self.critic_params = list(self.critic_body.parameters()) + list(self.fc_critic.parameters())\n",
    "        self.phi_params = list(self.phi_body.parameters())\n",
    "\n",
    "\n",
    "class GaussianActorCriticNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 state_dim,\n",
    "                 action_dim,\n",
    "                 phi_body=None,\n",
    "                 actor_body=None,\n",
    "                 critic_body=None):\n",
    "        super(GaussianActorCriticNet, self).__init__()\n",
    "        self.network = ActorCriticNet(state_dim, action_dim, phi_body, actor_body, critic_body)\n",
    "        self.std = nn.Parameter(torch.ones(1, action_dim))\n",
    "        self.to(DEVICE)\n",
    "\n",
    "    def forward(self, obs, action=None):\n",
    "        obs = tensor(obs)\n",
    "        phi = self.network.phi_body(obs)\n",
    "        phi_a = self.network.actor_body(phi)\n",
    "        phi_v = self.network.critic_body(phi)\n",
    "        mean = F.tanh(self.network.fc_action(phi_a))\n",
    "        v = self.network.fc_critic(phi_v)\n",
    "        dist = torch.distributions.Normal(mean, self.std)\n",
    "        if action is None:\n",
    "            action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        log_prob = torch.sum(log_prob, dim=1, keepdim=True)\n",
    "        return action, log_prob, tensor(np.zeros((log_prob.size(0), 1))), v\n",
    "    \n",
    "class FCBody(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_units=(64, 64), gate=F.relu):\n",
    "        super(FCBody, self).__init__()\n",
    "        dims = (state_dim, ) + hidden_units\n",
    "        self.layers = nn.ModuleList([layer_init(nn.Linear(dim_in, dim_out)) for dim_in, dim_out in zip(dims[:-1], dims[1:])])\n",
    "        self.gate = gate\n",
    "        self.feature_dim = dims[-1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = self.gate(layer(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "policy = GaussianActorCriticNet(state_size, action_size, actor_body=FCBody(state_size), critic_body=FCBody(state_size))\n",
    "optimizier = Adam(policy.parameters(), 3e-4, eps=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    \n",
    "class PPOAgent(BaseAgent):\n",
    "    \n",
    "    def __init__(self):\n",
    "        BaseAgent.__init__(self)\n",
    "        self.network = policy\n",
    "        self.opt = optimizier\n",
    "        self.total_steps = 0\n",
    "        self.online_rewards = np.zeros(num_agents)\n",
    "        self.episode_rewards = []\n",
    "        \n",
    "        env_info = env.reset(train_mode=True)[brain_name]    \n",
    "        self.states = env_info.vector_observations              \n",
    "\n",
    "    def step(self):\n",
    "        rollout = []\n",
    "        env_info = env.reset(train_mode=True)[brain_name]    \n",
    "        self.states = env_info.vector_observations  \n",
    "        states = self.states\n",
    "        for _ in range(ROLLOUT_LENGTH):\n",
    "            actions, log_probs, _, values = self.network(states)\n",
    "            env_info = env.step(actions.cpu().detach().numpy())[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            terminals = np.array([1 if t else 0 for t in env_info.local_done])\n",
    "            self.online_rewards += rewards\n",
    "            \n",
    "            for i, terminal in enumerate(terminals):\n",
    "                if terminals[i]:\n",
    "                    self.episode_rewards.append(self.online_rewards[i])\n",
    "                    self.online_rewards[i] = 0\n",
    "                    \n",
    "            rollout.append([states, values.detach(), actions.detach(), log_probs.detach(), rewards, 1 - terminals])\n",
    "            states = next_states\n",
    "\n",
    "        self.states = states\n",
    "        pending_value = self.network(states)[-1]\n",
    "        rollout.append([states, pending_value, None, None, None, None])\n",
    "\n",
    "        processed_rollout = [None] * (len(rollout) - 1)\n",
    "        advantages = tensor(np.zeros((num_agents, 1)))\n",
    "        returns = pending_value.detach()\n",
    "        for i in reversed(range(len(rollout) - 1)):\n",
    "            states, value, actions, log_probs, rewards, terminals = rollout[i]\n",
    "            terminals = tensor(terminals).unsqueeze(1)\n",
    "            rewards = tensor(rewards).unsqueeze(1)\n",
    "            actions = tensor(actions)\n",
    "            states = tensor(states)\n",
    "            next_value = rollout[i + 1][1]\n",
    "            returns = rewards + DISCOUNT_RATE * terminals * returns\n",
    "\n",
    "            td_error = rewards + DISCOUNT_RATE * terminals * next_value.detach() - value.detach()\n",
    "            advantages = advantages * TAU * DISCOUNT_RATE * terminals + td_error\n",
    "            processed_rollout[i] = [states, actions, log_probs, returns, advantages]\n",
    "\n",
    "        states, actions, log_probs_old, returns, advantages = map(lambda x: torch.cat(x, dim=0), zip(*processed_rollout))\n",
    "        advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "\n",
    "        batcher = Batcher(states.size(0) // MINI_BATCH_NUMBER, [np.arange(states.size(0))])\n",
    "        for _ in range(OPTIMIZATION_EPOCHS):\n",
    "            batcher.shuffle()\n",
    "            while not batcher.end():\n",
    "                batch_indices = batcher.next_batch()[0]\n",
    "                batch_indices = tensor(batch_indices).long()\n",
    "                sampled_states = states[batch_indices]\n",
    "                sampled_actions = actions[batch_indices]\n",
    "                sampled_log_probs_old = log_probs_old[batch_indices]\n",
    "                sampled_returns = returns[batch_indices]\n",
    "                sampled_advantages = advantages[batch_indices]\n",
    "\n",
    "                _, log_probs, entropy_loss, values = self.network(sampled_states, sampled_actions)\n",
    "                ratio = (log_probs - sampled_log_probs_old).exp()\n",
    "                obj = ratio * sampled_advantages\n",
    "                obj_clipped = ratio.clamp(1.0 - PPO_CLIP,\n",
    "                                          1.0 + PPO_CLIP) * sampled_advantages\n",
    "                policy_loss = -torch.min(obj, obj_clipped).mean(0) - ENTROPY_COEFICENT * entropy_loss.mean()\n",
    "\n",
    "                value_loss = 0.5 * (sampled_returns - values).pow(2).mean()\n",
    "\n",
    "                self.opt.zero_grad()\n",
    "                (policy_loss + value_loss).backward()\n",
    "                nn.utils.clip_grad_norm_(self.network.parameters(), GRADIENT_CLIP)\n",
    "                self.opt.step()\n",
    "\n",
    "        steps = ROLLOUT_LENGTH * num_agents\n",
    "        self.total_steps += steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Total score this episode: 0.181999995931983 Average 1: 0.181999995931983\n",
      "Episode: 1 Total score this episode: 0.11999999731779099 Average 2: 0.150999996624887\n",
      "Episode: 2 Total score this episode: 0.2369999947026372 Average 3: 0.17966666265080375\n",
      "Episode: 3 Total score this episode: 0.1599999964237213 Average 4: 0.17474999609403313\n",
      "Episode: 4 Total score this episode: 0.09699999783188104 Average 5: 0.1591999964416027\n",
      "Episode: 5 Total score this episode: 0.1289999971166253 Average 6: 0.15416666322077313\n",
      "Episode: 6 Total score this episode: 0.14399999678134917 Average 7: 0.15271428230085543\n",
      "Episode: 7 Total score this episode: 0.4079999908804893 Average 8: 0.18462499587330966\n",
      "Episode: 8 Total score this episode: 0.259499994199723 Average 9: 0.1929444401318\n",
      "Episode: 9 Total score this episode: 0.344499992299825 Average 10: 0.20809999534860252\n"
     ]
    }
   ],
   "source": [
    "agent = PPOAgent()\n",
    "all_scores = []\n",
    "for i in range(EPISODES_NUMBER):\n",
    "    agent.step()\n",
    "    \n",
    "    env_info = env.reset(train_mode=True)[brain_name]    \n",
    "    states = env_info.vector_observations                 \n",
    "    scores = np.zeros(num_agents)                         \n",
    "    while True:\n",
    "        actions, log_probs, _, values = policy(states)\n",
    "        env_info = env.step(actions.cpu().detach().numpy())[brain_name]\n",
    "        next_states = env_info.vector_observations         \n",
    "        rewards = env_info.rewards                         \n",
    "        dones = env_info.local_done                     \n",
    "        scores += env_info.rewards                      \n",
    "        states = next_states                               \n",
    "        if np.any(dones):                                  \n",
    "            break\n",
    "    m = np.mean(scores)\n",
    "    all_scores.append(m)\n",
    "    \n",
    "    if len(all_scores) > 100:\n",
    "        h = np.mean(np.array(all_scores[-100:]))\n",
    "    else:\n",
    "        h = np.mean(np.array(all_scores))\n",
    "    if h > 30.0:\n",
    "        agent.save(f\"ppo-over-30-episode-{i}\")\n",
    "    print('Episode: {} Total score this episode: {} Average {}: {}'.format(i + 1, m, min(i + 1, 100), h))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
