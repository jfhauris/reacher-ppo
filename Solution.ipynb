{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reacher - PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from unityagents import UnityEnvironment\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from model import PPOPolicyNetwork\n",
    "from agent import PPOAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Unity environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Reacher\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Unity environment parameters\n",
    "# STATE_SIZE = env_info.vector_observations.shape[1]\n",
    "# ACTION_SIZE = brain.vector_action_space_size\n",
    "# NUMBER_OF_AGENTS = len(env_info.agents)\n",
    "\n",
    "# # PyTorch device\n",
    "# DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Hyperparameters\n",
    "# DISCOUNT_RATE = 0.99\n",
    "# TAU = 0.95\n",
    "# GRADIENT_CLIP = 5\n",
    "# ROLLOUT_LENGTH = 2048\n",
    "# OPTIMIZATION_EPOCHS = 10\n",
    "# PPO_CLIP = 0.02\n",
    "# LOG_INTERVAL = 2048\n",
    "# MAX_STEPS = 2e7\n",
    "# MINI_BATCH_NUMBER = 32\n",
    "# ENTROPY_COEFICENT = 0.01\n",
    "# EPISODE_COUNT = 50\n",
    "# HIDDEN_SIZE = 512\n",
    "\n",
    "config = {\n",
    "    'environment': {\n",
    "        'state_size':  env_info.vector_observations.shape[1],\n",
    "        'action_size': brain.vector_action_space_size,\n",
    "        'number_of_agents': len(env_info.agents)\n",
    "    },\n",
    "    'pytorch': {\n",
    "        'device': torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    },\n",
    "    'hyperparameters': {\n",
    "        'discount_rate': 0.99,\n",
    "        'tau': 0.95,\n",
    "        'gradient_clip': 5,\n",
    "        'rollout_length': 2048,\n",
    "        'optimization_epochs': 10,\n",
    "        'ppo_clip': 0.02,\n",
    "        'log_interval': 2048,\n",
    "        'max_steps': 2e7,\n",
    "        'mini_batch_number': 32,\n",
    "        'entropy_coefficent': 0.01,\n",
    "        'episode_count': 20,\n",
    "        'hidden_size': 512,\n",
    "        'adam_learning_rate': 3e-4,\n",
    "        'adam_epsilon': 1e-5\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 20/20 [05:35<00:00, 17.07s/it]\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 20 Total score this episode: 0.6594999852590263 Last 20 average: 0.4708499894756824\n"
     ]
    }
   ],
   "source": [
    "policy = PPOPolicyNetwork(config)\n",
    "optimizier = optim.Adam(policy.parameters(), config['hyperparameters']['adam_learning_rate'], \n",
    "                        eps=config['hyperparameters']['adam_epsilon'])\n",
    "agent = PPOAgent(env, brain_name, policy, optimizier, config)\n",
    "\n",
    "\n",
    "all_scores = []\n",
    "for i in tqdm.tqdm(range(config['hyperparameters']['episode_count'])):\n",
    "    agent.step()\n",
    "    \n",
    "    env_info = env.reset(train_mode=True)[brain_name]    \n",
    "    states = env_info.vector_observations                 \n",
    "    scores = np.zeros(config['environment']['number_of_agents'])                         \n",
    "    while True:\n",
    "        actions, log_probs, _, values = policy(states)\n",
    "        env_info = env.step(actions.cpu().detach().numpy())[brain_name]\n",
    "        next_states = env_info.vector_observations         \n",
    "        rewards = env_info.rewards                         \n",
    "        dones = env_info.local_done                     \n",
    "        scores += env_info.rewards                      \n",
    "        states = next_states                               \n",
    "        if np.any(dones):                                  \n",
    "            break\n",
    "    m = np.mean(scores)\n",
    "    all_scores.append(m)\n",
    "    \n",
    "    if len(all_scores) > 100:\n",
    "        h = np.mean(np.array(all_scores[-100:]))\n",
    "    else:\n",
    "        h = np.mean(np.array(all_scores))\n",
    "    if h > 30.0:\n",
    "        torch.save(policy.state_dict(), f\"models/ppo-{h}-episode-{i}-hiddensize-{config['hyperparameters']['episode_count']['hidden_size']}.pth\")\n",
    "    clear_output(True)\n",
    "    print('Episode: {} Total score this episode: {} Last {} average: {}'.format(i + 1, m, min(i + 1, 100), h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
